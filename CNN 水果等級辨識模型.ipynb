{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「googlenetv4.ipynb」的副本",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAQnSxjumebF"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ajlk3nsn051jquf/best_gn4model_ep8_ac80.h5?dl=1\n",
        "!wget https://www.dropbox.com/s/ij04y6n0u1fmypf/easy_shallow_model_40030.h5?dl=1\n",
        "!wget https://www.dropbox.com/s/8ov0v435tm3sxkv/modelgn3%20.h5?dl=1\n",
        "!wget https://www.dropbox.com/s/kxou89giuwokjw7/best_meta_model2_retrain_gn4.h5?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaED1_QfnAQU"
      },
      "source": [
        "# **import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbqe3Y57yK6d"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import csv\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten,Dropout,BatchNormalization,AveragePooling2D,concatenate,Input, concatenate,LeakyReLU, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import initializers\n",
        "import tensorflow as tf    \n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow import keras   \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras import layers as layers\n",
        "from tensorflow.keras.layers import BatchNormalization as batchnorm\n",
        "from tensorflow.keras.utils import to_categorical as npcat\n",
        "from tensorflow.keras.backend import manual_variable_initialization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from sklearn.svm import SVC\n",
        "import xgboost as xgb\n",
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgRmOdf9y25a"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DHCZSrwkDhd"
      },
      "source": [
        "# **prepare data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTgofDrYj-7Z"
      },
      "source": [
        "!wget https://www.dropbox.com/s/yvu6qeu7m7insl5/data.zip?dl=1\n",
        "!unzip data.zip\\?dl\\=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4syx9qKakbXE"
      },
      "source": [
        "!wget https://www.dropbox.com/s/cobwr5hoxef7h5m/test.zip?dl=1\n",
        "!unzip test.zip?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_hldTPrEZGA"
      },
      "source": [
        "# **image enhance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmw1l0U5YmcE"
      },
      "source": [
        "from PIL import Image\n",
        "from PIL import ImageEnhance\n",
        "\n",
        "transform_type_dict = dict(\n",
        "    brightness=ImageEnhance.Brightness, contrast=ImageEnhance.Contrast,\n",
        "    sharpness=ImageEnhance.Sharpness,   color=ImageEnhance.Color\n",
        ")\n",
        "\n",
        "class ColorJitter(object):\n",
        "    def __init__(self, transform_dict):\n",
        "        self.transforms = [(transform_type_dict[k], transform_dict[k]) for k in transform_dict]\n",
        "    \n",
        "    def __call__(self, img):\n",
        "\n",
        "        img = Image.fromarray(img.astype(np.uint8))\n",
        "        out = img \n",
        "\n",
        "        if random.random() < 0.5:\n",
        "          rand_num = np.random.uniform(0, 1, len(self.transforms))\n",
        "          for i, (transformer, alpha) in enumerate(self.transforms):\n",
        "              r = alpha * (rand_num[i]*2.0 - 1.0) + 1   # r in [1-alpha, 1+alpha)\n",
        "              out = transformer(out).enhance(r)\n",
        "        out = np.array(out,dtype=np.float32)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP5PkoVVEfZL"
      },
      "source": [
        "_transform_dict = {'brightness':0.1026, 'contrast':0.0935, 'sharpness':0.8386, 'color':0.1592}\n",
        "_color_jitter = ColorJitter(_transform_dict)\n",
        "img_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    preprocessing_function=_color_jitter\n",
        "    )\n",
        "test_gen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5khfYVZPsqsS"
      },
      "source": [
        "# **Load Data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXETITmWrMfb"
      },
      "source": [
        "## **make list**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7NZtaYNzh6k"
      },
      "source": [
        "train = open('/content/data/train.csv')\n",
        "val = open('/content/data/dev.csv')\n",
        "\n",
        "reader = csv.reader(train)\n",
        "labels = []\n",
        "label={}\n",
        "for line in reader:\n",
        "  tmp = [line[0],line[1]]\n",
        "  labels.append(tmp)\n",
        "train.close() \n",
        "ls1=[]\n",
        "ls2=[]\n",
        "ls3=[]\n",
        "ls4=[]\n",
        "partition = {}\n",
        "for i in range(1,len(labels)):\n",
        "  labels[i][1] = labels[i][1].replace(\"A\",\"0\")\n",
        "  labels[i][1] = labels[i][1].replace(\"B\",\"1\")\n",
        "  labels[i][1] = labels[i][1].replace(\"C\",\"2\")\n",
        "  if random.random() <= 0.2:\n",
        "    if random.random() <= 0.2:ls4.append('train/'+labels[i][0])\n",
        "    else:ls2.append('train/'+labels[i][0])\n",
        "  else:ls1.append('train/'+labels[i][0])\n",
        "  label['train/'+labels[i][0]]=labels[i][1]\n",
        "\n",
        "\n",
        "reader = csv.reader(val)\n",
        "labels = []\n",
        "for line in reader:\n",
        "  tmp = [line[0],line[1]]\n",
        "  labels.append(tmp)\n",
        "val.close()\n",
        "\n",
        "for i in range(1,len(labels)):\n",
        "  labels[i][1] = labels[i][1].replace(\"A\",\"0\")\n",
        "  labels[i][1] = labels[i][1].replace(\"B\",\"1\")\n",
        "  labels[i][1] = labels[i][1].replace(\"C\",\"2\")\n",
        "  ls3.append('dev/'+labels[i][0])\n",
        "  label['dev/'+labels[i][0]]=labels[i][1]\n",
        "\n",
        "partition['train']=ls1\n",
        "partition['dev']=ls2\n",
        "partition['test']=ls3\n",
        "partition['v_val']=ls4\n",
        "partition['combine']=ls1+ls2+ls4\n",
        "partition['vcom']=ls2+ls4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZVE4t3X2g1W"
      },
      "source": [
        "## **data loader**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhbgvbS4zcaV"
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(400,400), n_channels=3,\n",
        "                 n_classes=3, shuffle=False,blend=False, aug=True):\n",
        "        'Initialization'\n",
        "        self.blend=blend\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.aug=aug\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels),dtype=np.float32)\n",
        "        y = np.empty((self.batch_size), dtype=np.float32)\n",
        "        imsize=400\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            img = cv2.imread('/content/data/' + ID)\n",
        "            res = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2RGB),(imsize,imsize),interpolation=cv2.INTER_LINEAR)\n",
        "            t=img_to_array(res)\n",
        "            X[i,] = img_to_array(res/1.)\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        y = npcat(y, self.n_classes)\n",
        "        if self.aug==True:\n",
        "          a=img_gen.flow(X, y, batch_size = 32)\n",
        "        else:\n",
        "          a=test_gen.flow(X, y, batch_size = 32)\n",
        "        X=a[0][0]\n",
        "        y=a[0][1]\n",
        "        if self.blend:return [X,X,X,X,X,X],y\n",
        "        else:return X,y\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, batch_size=32, dim=(400,400), n_channels=3,\n",
        "                 n_classes=3, shuffle=False,blend=False, aug=True):\n",
        "        'Initialization'\n",
        "        self.blend=blend\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.aug=aug\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels),dtype=np.float32)\n",
        "        y = np.empty((self.batch_size), dtype=np.float32)\n",
        "        imsize=400\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            img = cv2.imread('/content/data/' + ID)\n",
        "            res = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2RGB),(imsize,imsize),interpolation=cv2.INTER_LINEAR)\n",
        "            t=img_to_array(res)\n",
        "            X[i,] = img_to_array(res/1.)\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        y = npcat(y, self.n_classes)\n",
        "        if self.aug==True:\n",
        "          a=img_gen.flow(X, y, batch_size = 32)\n",
        "        else:\n",
        "          a=test_gen.flow(X, y, batch_size = 32)\n",
        "        X=a[0][0]\n",
        "        y=a[0][1]\n",
        "        if self.blend:return [X,X,X,X,X,X],y\n",
        "        else:return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuaiEE552r7d"
      },
      "source": [
        "## **set parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_17j8jC80G-T"
      },
      "source": [
        "# Parameters\n",
        "train_params = {'dim': (400,400),\n",
        "                'batch_size': 32,\n",
        "                'n_classes': 3,\n",
        "                'n_channels': 3,\n",
        "                'shuffle': True,\n",
        "                'blend':False,\n",
        "                'aug':True}\n",
        "\n",
        "test_params = {'dim': (400,400),\n",
        "              'batch_size': 32,\n",
        "              'n_classes': 3,\n",
        "              'n_channels': 3,\n",
        "              'shuffle': False,\n",
        "              'blend':False,\n",
        "              'aug':False}\n",
        "\n",
        "blend_params = {'dim': (400,400),\n",
        "              'batch_size': 32,\n",
        "              'n_classes': 3,\n",
        "              'n_channels': 3,\n",
        "              'shuffle': True,\n",
        "              'blend':True,\n",
        "              'aug':True}\n",
        "\n",
        "vval_params = {'dim': (400,400),\n",
        "              'batch_size': 32,\n",
        "              'n_classes': 3,\n",
        "              'n_channels': 3,\n",
        "              'shuffle': False,\n",
        "              'blend':True,\n",
        "              'aug':False}\n",
        "\n",
        "# Generators\n",
        "training_generator = DataGenerator(partition['train'], label, **train_params)\n",
        "validation_generator = DataGenerator(partition['vcom'], label, **test_params)\n",
        "train_val_generator = DataGenerator(partition['combine'], label, **train_params)\n",
        "testing_generator = DataGenerator(partition['test'], label, **test_params)\n",
        "blending_generator = DataGenerator(partition['test'], label, **blend_params)\n",
        "vval = DataGenerator(partition['v_val'], label, **vval_params)\n",
        "vc = DataGenerator(partition['vcom'], label, **blend_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B6JTLPkGWoe"
      },
      "source": [
        "# **Convolution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4_Qnr8nGcMS"
      },
      "source": [
        "def conv2d(net, filters, kernel_size, strides=(1, 1), padding='same'):\n",
        "    net = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,use_bias=False)(net)\n",
        "    net = BatchNormalization(axis=3)(net)\n",
        "    net = LeakyReLU(alpha=0.1126)(net)\n",
        "    # net = Activation('relu')(net)\n",
        "    return net"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXDwyOqXGI-r"
      },
      "source": [
        "#**blocks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHOki7h7GAbp"
      },
      "source": [
        "def block_stem(inputs):\n",
        "    net = conv2d(inputs, 32, (3, 3), strides=(2, 2), padding='valid')\n",
        "    net = conv2d(net, 32, (3, 3), padding='valid')\n",
        "    net = conv2d(net, 64, (3, 3))\n",
        "    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(net)\n",
        "    branch_2 = conv2d(net, 96, (3, 3), strides=(2, 2), padding='valid')\n",
        "    net = concatenate([branch_1, branch_2])\n",
        "    branch_1 = conv2d(net, 64, (1, 1))\n",
        "    branch_1 = conv2d(branch_1, 96, (3, 3), padding='valid')\n",
        "    branch_2 = conv2d(net, 64, (1, 1))\n",
        "    branch_2 = conv2d(branch_2, 64, (7, 1))\n",
        "    branch_2 = conv2d(branch_2, 64, (1, 7))\n",
        "    branch_2 = conv2d(branch_2, 96, (3, 3), padding='valid')\n",
        "    net = concatenate([branch_1, branch_2])\n",
        "    branch_1 = conv2d(net, 192, (3, 3), strides=(2, 2), padding='valid')  # different from the paper\n",
        "    branch_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(net)\n",
        "    net = concatenate([branch_1, branch_2])\n",
        "    return net\n",
        "\n",
        "\n",
        "def block_inception_a(inputs):\n",
        "    branch_1 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_1 = conv2d(branch_1, 96, (1, 1))\n",
        "\n",
        "    branch_2 = conv2d(inputs, 96, (1, 1))\n",
        "\n",
        "    branch_3 = conv2d(inputs, 64, (1, 1))\n",
        "    branch_3 = conv2d(branch_3, 96, (3, 3))\n",
        "\n",
        "    branch_4 = conv2d(inputs, 64, (1, 1))\n",
        "    branch_4 = conv2d(branch_4, 96, (3, 3))\n",
        "    branch_4 = conv2d(branch_4, 96, (3, 3))\n",
        "\n",
        "    return concatenate([branch_1, branch_2, branch_3, branch_4])\n",
        "\n",
        "\n",
        "def block_inception_b(inputs):\n",
        "    branch_1 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_1 = conv2d(branch_1, 128, (1, 1))\n",
        "\n",
        "    branch_2 = conv2d(inputs, 384, (1, 1))\n",
        "\n",
        "    branch_3 = conv2d(inputs, 192, (1, 1))\n",
        "    branch_3 = conv2d(branch_3, 224, (1, 7))\n",
        "    branch_3 = conv2d(branch_3, 256, (7, 1))  # different from the paper\n",
        "\n",
        "    branch_4 = conv2d(inputs, 192, (1, 1))\n",
        "    branch_4 = conv2d(branch_4, 192, (1, 7))\n",
        "    branch_4 = conv2d(branch_4, 224, (7, 1))\n",
        "    branch_4 = conv2d(branch_4, 224, (1, 7))\n",
        "    branch_4 = conv2d(branch_4, 256, (7, 1))\n",
        "\n",
        "    return concatenate([branch_1, branch_2, branch_3, branch_4])\n",
        "\n",
        "\n",
        "def block_inception_c(inputs):\n",
        "    branch_1 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(inputs)\n",
        "    branch_1 = conv2d(branch_1, 256, (1, 1))\n",
        "\n",
        "    branch_2 = conv2d(inputs, 256, (1, 1))\n",
        "\n",
        "    branch_3 = conv2d(inputs, 384, (1, 1))\n",
        "    branch_3_1 = conv2d(branch_3, 256, (1, 3))\n",
        "    branch_3_2 = conv2d(branch_3, 256, (3, 1))\n",
        "\n",
        "    branch_4 = conv2d(inputs, 384, (1, 1))\n",
        "    branch_4 = conv2d(branch_4, 448, (1, 3))\n",
        "    branch_4 = conv2d(branch_4, 512, (3, 1))\n",
        "    branch_4_1 = conv2d(branch_4, 256, (3, 1))\n",
        "    branch_4_2 = conv2d(branch_4, 256, (1, 3))\n",
        "\n",
        "    return concatenate([branch_1, branch_2, branch_3_1, branch_3_2, branch_4_1, branch_4_2])\n",
        "\n",
        "\n",
        "def block_reduction_a(inputs):\n",
        "    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(inputs)\n",
        "\n",
        "    branch_2 = conv2d(inputs, 384, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    branch_3 = conv2d(inputs, 192, (1, 1))\n",
        "    branch_3 = conv2d(branch_3, 224, (3, 3))\n",
        "    branch_3 = conv2d(branch_3, 256, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    return concatenate([branch_1, branch_2, branch_3])\n",
        "\n",
        "\n",
        "def block_reduction_b(inputs):\n",
        "    branch_1 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(inputs)\n",
        "\n",
        "    branch_2 = conv2d(inputs, 192, (1, 1))\n",
        "    branch_2 = conv2d(branch_2, 192, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    branch_3 = conv2d(inputs, 256, (1, 1))\n",
        "    branch_3 = conv2d(branch_3, 256, (1, 7))\n",
        "    branch_3 = conv2d(branch_3, 320, (7, 1))\n",
        "    branch_3 = conv2d(branch_3, 320, (3, 3), strides=(2, 2), padding='valid')\n",
        "\n",
        "    return concatenate([branch_1, branch_2, branch_3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWzk6frKFeLM"
      },
      "source": [
        "# **create_model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXe9jwt3Fcwx"
      },
      "source": [
        "def create(classes_num, image_height, image_width, image_channel):\n",
        "    inputs = Input((image_height, image_width, image_channel))\n",
        "    net = block_stem(inputs)\n",
        "    for i in range(2):\n",
        "        net = block_inception_a(net)\n",
        "    net = block_reduction_a(net)\n",
        "    for i in range(2):\n",
        "        net = block_inception_b(net)\n",
        "    net = block_reduction_b(net)\n",
        "    for i in range(3):\n",
        "        net = block_inception_c(net)\n",
        "    net = AveragePooling2D((8, 8))(net)\n",
        "    net = Dropout(0.2)(net)\n",
        "    net = Flatten()(net)\n",
        "    # Output\n",
        "    x=Dense(1000,kernel_initializer=initializers.he_uniform())(net)\n",
        "    x = LeakyReLU(alpha=0.1126)(x)\n",
        "    outputs = Dense(units=classes_num, activation='softmax',kernel_initializer=initializers.he_uniform())(x)\n",
        "    return Model(inputs, outputs, name='Inception-v4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VASLPqBlr4tS"
      },
      "source": [
        "# **training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHf2TzRXHxno"
      },
      "source": [
        "imsize=400\n",
        "model = create(3,imsize,imsize,3)\n",
        "CB = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint('shallow_modelgn4_20_best.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "model.compile(optimizer=Adam(lr=1e-3),loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
        "# History = model.fit(img_gen.flow(x_train, y_train, batch_size = 4),steps_per_epoch = len(x_train)/4,validation_data = (x_test/255,y_test), epochs = 30 )\n",
        "History = model.fit_generator(generator=training_generator,validation_data=validation_generator, epochs = 30,callbacks=[es,mc] )\n",
        "model.save('shallow_modelgn4.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVwdQiJPIrkM"
      },
      "source": [
        "## **eval_v4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHwcVkywOIxM"
      },
      "source": [
        "model1=load_model('shallow_modelgn4_20_best.h5')\n",
        "model1.evaluate_generator(generator=testing_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_7fTSIX3haQ"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint('best_gn4model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "hist = model1.fit_generator(generator=training_generator,validation_data=validation_generator, epochs = 10,callbacks=[es,mc])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6pEcXVd29x9"
      },
      "source": [
        "# **keras v3 pretrain**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU8hT4MET2uT"
      },
      "source": [
        "# create the base pre-trained model\n",
        "input_tensor = Input(shape=(400, 400, 3))\n",
        "base_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)# False不訓練最後的dense\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "#最後幾層fully connected\n",
        "x = Dropout(0.2)(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(1024,kernel_initializer=initializers.he_uniform())(x)\n",
        "x = LeakyReLU(alpha=0.2)(x)\n",
        "predictions = Dense(3,activation='softmax')(x)\n",
        "model = Model(input_tensor, outputs=predictions)\n",
        "\n",
        "#只train最後幾層，其他層不動\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer=Adam(lr=1e-3),loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
        "#train最後幾層用的\n",
        "# model.fit(img_gen.flow(x_train, y_train, batch_size = 4),steps_per_epoch = len(x_train)/4,validation_data = (x_test/255,y_test), epochs = 3 )\n",
        "model.fit_generator(generator=training_generator,validation_data=validation_generator, epochs = 5 )\n",
        "\n",
        "for layer in model.layers[:249]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[249:]:\n",
        "   layer.trainable = True\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer=Adam(lr=1e-3),loss = 'categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "# fine-tuning整個model\n",
        "History = model.fit_generator(generator=training_generator,validation_data=validation_generator, epochs = 20 )\n",
        "model.save('modelgn3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt1yFpEEMAZN"
      },
      "source": [
        "## **eval_v3**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVwyanTU_C2P"
      },
      "source": [
        "model1=load_model('modelgn3.h5')\n",
        "model1.evaluate_generator(generator=testing_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLJCQ30x7bnX"
      },
      "source": [
        "model1.save('modelgn3.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91waHJR6axAL"
      },
      "source": [
        "# **blending**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJOSsqNAajHx"
      },
      "source": [
        "def load_all_models():\n",
        "    all_models = []\n",
        "    model1=load_model('best_gn4model_ep8_ac80.h5')\n",
        "    model2=load_model('modelgn3.h5')\n",
        "    model3=load_model('easy_shallow_model_40030.h5')\n",
        "    model4=load_model('best_meta_model_1_level.h5')\n",
        "    all_models.append(model1)\n",
        "    all_models.append(model2)\n",
        "    all_models.append(model3)\n",
        "    all_models.append(model4)\n",
        "    return all_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSnegJAmhu0A"
      },
      "source": [
        "def define_stacked_model(members):\n",
        "\t# update all layers in all models to not be trainable\n",
        "\tfor i in range(len(members)):\n",
        "\t\tmodel = members[i]\n",
        "\t\tfor layer in model.layers:\n",
        "\t\t\t# make not trainable\n",
        "\t\t\tlayer.trainable = False\n",
        "\t\t\t# rename to avoid 'unique layer name' issue\n",
        "\t\t\tlayer._name = 'ensemble_' + str(i+1) + '_' + str(random.random())\n",
        "\t# define multi-headed input\n",
        "\tensemble_visible = [model.input for model in members]\n",
        "\t# concatenate merge output from each model\n",
        "\tensemble_outputs = [model.output for model in members]\n",
        "\tmerge = concatenate(ensemble_outputs)\n",
        "\tx = Dense(512,kernel_initializer=initializers.he_uniform())(merge)\n",
        "\tx = Dropout(0.15)(x)\n",
        "\tx = LeakyReLU(alpha=0.11)(x)# maybe dropout\n",
        "\toutput = Dense(3, activation='softmax',kernel_initializer=initializers.he_uniform())(x)\n",
        "\tmodel = Model(inputs=ensemble_visible, outputs=output)\n",
        "\tmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCO_fqScK2bD"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "model=load_model('best_meta_model_2_level.h5')\n",
        "plot_model(model,  to_file='model_graph.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjz7uKUtnlVo"
      },
      "source": [
        "from keras.utils import plot_model\n",
        "members=load_all_models()\n",
        "stacked_model = define_stacked_model(members)\n",
        "es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint('best_meta_model_2_level.h5', monitor='accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "hist = stacked_model.fit_generator(generator=blending_generator, epochs = 30,callbacks=[es,mc])\n",
        "stacked_model.save('learning_blending1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfBmG1KpcfQd"
      },
      "source": [
        "# **SVM data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gJIV6D2zGPD"
      },
      "source": [
        "fortest = DataGenerator(partition['test'], label, **vval_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FHPGFpYwwZK"
      },
      "source": [
        "model=load_model('best_meta_model_2_level_retraingn4.h5')\n",
        "model.evaluate_generator(generator=fortest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q76tXGtKyJby"
      },
      "source": [
        "# **SVM**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD9GnLNLyNEQ"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ded8ftr922jmkwr/meta_learning_blending.h5?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWdfUjBAJQVy"
      },
      "source": [
        "blend_params = {'dim': (400,400),\n",
        "              'batch_size': 32,\n",
        "              'n_classes': 3,\n",
        "              'n_channels': 3,\n",
        "              'shuffle': False,\n",
        "              'blend':True,\n",
        "              'aug':False}\n",
        "train_params = {'dim': (400,400),\n",
        "              'batch_size': 32,\n",
        "              'n_classes': 3,\n",
        "              'n_channels': 3,\n",
        "              'shuffle': True,\n",
        "              'blend':True,\n",
        "              'aug':True}\n",
        "blending = DataGenerator(partition['test'], label, **blend_params)\n",
        "training = DataGenerator(partition['vcom'], label, **train_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PWYYRH2_kex"
      },
      "source": [
        "def getmodelfeature(model):\n",
        "  model_feat = Model(inputs=model.input,outputs=model.get_layer('dense_6').output)\n",
        "  return model_feat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTlx0DDBClkc"
      },
      "source": [
        "def get_traindata(generator,model_feat):\n",
        "  data = [x for x in generator]\n",
        "  feature=[]\n",
        "  target=[]\n",
        "  for i in range(len(data)):\n",
        "    pred=model_feat.predict(data[i][0])\n",
        "    feature.append(pred)\n",
        "    target.append(data[i][1])\n",
        "\n",
        "  feat_train=concatenate([feature[0],feature[1]],axis=0)\n",
        "  tar_train=concatenate([target[0],target[1]],axis=0)\n",
        "\n",
        "  for i in range(2,len(feature)):\n",
        "    feat_train=concatenate([feat_train,feature[i]],axis=0)\n",
        "    tar_train=concatenate([tar_train,target[i]],axis=0)\n",
        "  return feat_train,tar_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_ppmhwuDklX"
      },
      "source": [
        "def get_svmscore(feature,f_y,test,t_y):\n",
        "  svm = SVC(kernel='poly')\n",
        "  svm.fit(feature,np.argmax(f_y,axis=1))\n",
        "  return svm.score(test,np.argmax(t_y,axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMe1SzlAL_6V"
      },
      "source": [
        "def get_xgbscore(feature,f_y,test,t_y):\n",
        "  xb = xgb.XGBClassifier()\n",
        "  xb.fit(feature,np.argmax(f_y,axis=1))\n",
        "  return xb.score(test,np.argmax(t_y,axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDfiQYU5TnqM"
      },
      "source": [
        "# model=load_model('meta_learning_blending.h5')\n",
        "def test_model(model):\n",
        "  model_feat=getmodelfeature(model)\n",
        "  feat_train,tar_train=get_traindata(training,model_feat)\n",
        "  feat_test,tar_test=get_traindata(blending,model_feat)\n",
        "  svm_score = get_svmscore(feat_train,tar_train,feat_test,tar_test)\n",
        "  # xgb_score = get_xgbscore(feat_train,tar_train,feat_test,tar_test)\n",
        "  print(svm_score)\n",
        "  # print(xgb_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCj-7vm66aEH"
      },
      "source": [
        " test_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmW9C7m5iODJ"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ajlk3nsn051jquf/best_gn4model_ep8_ac80.h5?dl=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n5BOTY05Q4A"
      },
      "source": [
        "# **Plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjhyfp0n5RXv"
      },
      "source": [
        "plt.figure(figsize = (15,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(hist.history['accuracy'])\n",
        "# plt.plot(hist.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(hist.history['loss'])\n",
        "# plt.plot(History.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}